import { Callout } from 'nextra/components'
import { Tabs } from 'nextra/components'

# Models

Welcome to the Rhesis SDK Models documentation. Models are core components of the SDK that enable you to create and manage test sets for Gen AI applications.

Models also play a crucial role in the evaluation process, serving as LLM judges to assess and validate outputs.


## How to use models

### Using the `get_model` function

The easiest way to use models is through the `get_model` function. When called without any arguments, this function returns the default Rhesis model.

```python
from rhesis.sdk.models import get_model

model = get_model()
```

To use a different provider, you can pass the provider name as an argument. This will use the default model for that provider.
```python
from rhesis.sdk.models import get_model

# Use default Gemini model
model = get_model("gemini")
```

<Callout type="info">
The following provider names can be passed as arguments to the `get_model` function:
- `gemini` - Google's AI models with multimodal support
- `openai` - OpenAI models including 
- `huggingface` - Open-source models from Hugging Face
- `ollama` - Local model execution using Ollama
- `rhesis` - Models served by Rhesis
</Callout>

To use a specific model, provide its name in the format `provider/model_name`. For example, the following code uses the Gemini model `gemini-2.0-flash`:

```python
from rhesis.sdk.models import get_model

model = get_model("gemini/gemini-2.0-flash")
```

The above code is equivalent to:

```python
from rhesis.sdk.models import get_model

model = get_model(provider="gemini", model_name="gemini-2.0-flash")
```

### Direct import

Alternatively, you can access models by importing the model class directly. When you provide a model name as an argument, that specific model will be used. If no model name is provided, the default model for that provider will be used.


```python
from rhesis.sdk.models import GeminiLLM, OllamaLLM

# Use default Ollama model
ollama_model = OllamaLLM()

# Use specific Gemini model
gemini_model = GeminiLLM('gemini-2.0-flash')
```

## How to generate content with models

All models share a consistent interface. The primary function is `generate`, which accepts the following arguments:

- `prompt`: The text prompt for generation
- `schema`: (optional) A Pydantic schema defining the structure of the generated text

### Generate text using prompt only

When you provide only a prompt, the model will generate text based on that input.

<Tabs items={['Code', 'Output']}>
<Tabs.Tab>
```python
# Use default Rhesis model
model = get_model()
output = model.generate(prompt="What is the capital of France?")
```
</Tabs.Tab>
<Tabs.Tab>
```
The capital of France is Paris.
```
</Tabs.Tab>
</Tabs>

### Generate structured output using schemas

You can specify schemas for structured generation. The generated output will be returned as a dictionary matching the defined schema structure.

<Tabs items={['Code', 'Output']}>
<Tabs.Tab>
```python
from pydantic import BaseModel
from rhesis.sdk.models import get_model

class City(BaseModel):
    name: str
    population: int


class CityResponse(BaseModel):
    biggest_cities: list[City]

# Use default Rhesis model
model = get_model()
output = model.generate(
    prompt="The list of 5 biggest cities in Germany?", schema=CityResponse
)
```
</Tabs.Tab>
<Tabs.Tab>
```json
{
  "biggest_cities": [
    { "name": "Berlin", "population": 3677472 },
    { "name": "Hamburg", "population": 1899151 },
    { "name": "Munich", "population": 1588218 },
    { "name": "Cologne", "population": 1164038 },
    { "name": "Frankfurt", "population": 763380 }
  ]
}
```
</Tabs.Tab>
</Tabs>

## Using models with synthesizers and metrics

Models become most useful when combined with synthesizers and metrics. Both synthesizers and metrics accept a `model` parameter that can be either:

- A model instance (created with `get_model()`)
- A model name string (e.g., `"gemini"` or `"gemini/gemini-2.0-flash"`)

If no model is specified, the default Rhesis model is used.

### With synthesizers

```python
from rhesis.sdk.models import get_model
from rhesis.sdk.synthesizers import PromptSynthesizer

# Using a model instance
model = get_model("gemini")
synthesizer = PromptSynthesizer(
    prompt="Generate tests for the car selling chatbot", 
    model=model
)

# Using a model name directly
synthesizer = PromptSynthesizer(
    prompt="Generate tests for the insurance chatbot", 
    model="gemini/gemini-2.0-flash"
)
```

### With metrics

```python
from rhesis.sdk.metrics import RhesisPromptMetricNumeric

# Using a model name
metric = RhesisPromptMetricNumeric(
    name="answer_quality_evaluator",
    evaluation_prompt="""
    Evaluate the answer for accuracy, completeness, clarity, and relevance.
    """,
    model="gemini"
)