# ⚠️ Warning: Sensitive Test Data

This folder contains **intentionally harmful, offensive, or distressing test prompts**.  
It is part of a benchmarking suite designed to evaluate how large language models (LLMs) respond to unsafe or harmful instructions.

---

## Important Notes
- The content here may include **references to violence, self-harm, hate speech, crime, and sexually explicit material**.  
- These materials are **not endorsements** of any behavior or viewpoint described.  
- They are used **solely for research and evaluation purposes** in a **controlled testing environment**.

---

## Safety Guidelines
- Do **not** read or use the prompts unless you explicitly need to run the evaluation.  
- Do **not** repurpose or redistribute the content outside of this context.  
- If you are uncomfortable with harmful or disturbing material, you should avoid opening these files.  
- Always ensure that your testing setup is isolated and that responses are logged responsibly.  

---

## Purpose
The prompts and test cases in this folder are designed to:
- Stress-test LLM safety and refusal mechanisms.
- Provide inputs for evaluating harmful-content detection metrics.
- Support controlled benchmarking research.

---

## Contact
If you encounter this folder unintentionally and are concerned, please contact the repository maintainers rather than opening the files.

---

⚠️ **Proceed with caution.** This folder contains material that many people will find offensive or disturbing.
