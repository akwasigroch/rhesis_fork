# Metrics

## Overview

The Rhesis SDK provides a comprehensive metrics system for evaluating LLM-based systems. The metrics module supports multiple evaluation frameworks and allows you to create custom metrics tailored to your specific use cases. 
The metrics module is integrated with the backend allowing you to work with metrics directly from the platform.


<Callout type="info">
The SDK integrates with popular evaluation frameworks:

- **DeepEval**
- **Ragas**
- **Rhesis**

All framerowkrs are used by the common interface which allows you to use the metrics from different frameworks using the same interface.
</Callout>

## Supported metrics

### DeepEval Metrics

| Metric | Description | Requires Context | Requires Ground Truth |
|--------|-------------|------------------|----------------------|
| `DeepEvalAnswerRelevancy` | Measures answer relevance to the question | No | No |
| `DeepEvalFaithfulness` | Checks if answer is grounded in context | Yes | No |
| `DeepEvalContextualRelevancy` | Evaluates context relevance to question | Yes | No |
| `DeepEvalContextualPrecision` | Measures precision of retrieved context | Yes | Yes |
| `DeepEvalContextualRecall` | Measures recall of retrieved context | Yes | Yes |
| `DeepEvalBias` | Detects biased content | No | No |
| `DeepEvalToxicity` | Detects toxic content | No | No |
| `DeepEvalPIILeakage` | Detects personally identifiable information | No | No |

### Ragas Metrics

| Metric | Description | Requires Context | Requires Ground Truth |
|--------|-------------|------------------|----------------------|
| `RagasAnswerAccuracy` | Measures answer accuracy | No | Yes |
| `RagasContextRelevance` | Evaluates context relevance | Yes | No |
| `RagasFaithfulness` | Checks answer groundedness | Yes | No |
| `RagasAspectCritic` | Custom aspect-based evaluation | No | No |

### Rhesis Custom Metrics

| Metric | Description | Configuration |
|--------|-------------|--------------|
| `NumericJudge` | LLM-based numeric scoring | Min/max score, threshold, custom prompts |
| `CategoricalJudge` | LLM-based categorical classification | Categories, passing categories, custom prompts |


If some metrics is missing from the list, or you would like to use a provider, please let us know 
by creating an issue on [GitHub](https://github.com/rhesis-ai/rhesis/issues).

## Quick Start

### Using DeepEval Metrics

```python
from rhesis.sdk.metrics import DeepEvalAnswerRelevancy

# Initialize metric
metric = DeepEvalAnswerRelevancy(threshold=0.7)

# Evaluate
result = metric.evaluate(
    input="What is the capital of France?",
    output="The capital of France is Paris."
)

print(f"Score: {result.score}")
print(f"Passed: {result.details['is_successful']}")
```

### Using Ragas Metrics

```python
from rhesis.sdk.metrics import RagasFaithfulness

# Initialize metric
metric = RagasFaithfulness(threshold=0.8)

# Evaluate with context
result = metric.evaluate(
    input="What is photosynthesis?",
    output="Photosynthesis is the process by which plants convert light "
          "into energy.",
    context=["Photosynthesis occurs in chloroplasts...", 
             "Plants use sunlight..."]
)

print(f"Score: {result.score}")
```

### Creating Custom Metrics

You can create custom metrics using the `NumericJudge` and `CategoricalJudge` classes.
`numericjudge` return the score in the form of the number (for example from 0 to 1), while `categoricaljudge` return the score in the form of the category (for example "good", "fair", "poor").

Each of this need to specify `evaluation_prompt`. Foe better restuls you can specify `evalution steps`, `reasoning` and `evaluation_examples`.

#### Numeric Judge

Numeric judge needs 4 specific parameters to be defined: `min_score`, `max_score`, `threshold` and `threshold_operator`.

```python
from rhesis.sdk.metrics import NumericJudge

# Define custom numeric metric
metric = NumericJudge(
    name="response_clarity",
    evaluation_prompt="Rate how clear and understandable the response is.",
    evaluation_steps="1. Check sentence structure\n"
                     "2. Evaluate word choice\n"
                     "3. Assess overall clarity",
    min_score=0.0,
    max_score=10.0,
    threshold=7.0
)

# Evaluate
result = metric.evaluate(
    input="Explain quantum computing",
    output="Quantum computers use qubits to process information...",
    expected_output="A quantum computer uses quantum mechanics..."
)
```

#### Categorical Judge

Categorical judge needs to specify `categories` and `passing_categories`.

```python
from rhesis.sdk.metrics import CategoricalJudge

# Define custom categorical metric
metric = CategoricalJudge(
    name="tone_classifier",
    evaluation_prompt="Classify the tone of the response.",
    categories=["professional", "casual", "technical", "friendly"],
    passing_categories=["professional", "technical"]
)

# Evaluate
result = metric.evaluate(
    input="Describe machine learning",
    output="Machine learning is a subset of AI...",
    expected_output="ML enables systems to learn from data..."
)

print(f"Category: {result.score}")
print(f"Passed: {result.details['is_successful']}")
```

## Loading Metrics from configuration
Custom metrics can be serialized or deserialized. To do so use `from_config` and `to_config` methods, which operate using config dataclass. You can also use `from_dict` and `to_dict` methods.

```python
metric = NumericJudge(
    name="response_clarity",
    evaluation_prompt="Rate how clear and understandable the response is.",
    evaluation_steps="1. Check sentence structure\n"
                     "2. Evaluate word choice\n"
                     "3. Assess overall clarity",
    min_score=0.0,
    max_score=10.0,
    threshold=7.0
)

config = metric.to_config()
metric = NumericJudge.from_config(config)
```

## Use metrics in the platform

The work on metrics can be done in the platform and in the sdk. To make it easier, the sdk provides 
`push` and `pull` methods to push and pull metrics to and from the platform.
To do so, you need to specify the following attributes, which are used by our platform:

```python
metric = NumericJudge(
    name="response_clarity",
    description="Rate how clear and understandable the response is.",
    metric_type="classification",
    requires_ground_truth=True,
    requires_context=False
    evaluation_prompt="Rate how clear and understandable the response is.",
    evaluation_steps="1. Check sentence structure\n"
                     "2. Evaluate word choice\n"
                     "3. Assess overall clarity",
    min_score=0.0,
    max_score=10.0,
    threshold=7.0
)
metric.push()
```

To pull the metrics from the platform, you can use the `pull` method. You need to specify the metric name. If the name is not unique, you have to specify the metric id.

```python
metric = NumericJudge.pull(name="response_clarity")
```

## Metric Results

All metrics return a `MetricResult` object:

```python
result = metric.evaluate(input="...", output="...")

# Access score
# Numeric score or categorical value
print(result.score)

# Access details
print(result.details)
# {
#     'score': 0.85,
#     'reason': 'The response is highly relevant...',
#     'is_successful': True,
#     'threshold': 0.7,
#     'score_type': 'numeric'
# }
```

## Models

All metrics need the LLM model to do the evaluation. If the model is not specified, the default model will be used. You can specify the model in the `model` argument. You can iether pass the model object or the model name.
For more information about models, see [Models Documentation](./models.mdx).

```python
from rhesis.sdk.metrics import DeepEvalAnswerRelevancy
from rhesis.sdk.models import get_model

# Use specific model
model = get_model("gemini")
metric = DeepEvalAnswerRelevancy(threshold=0.7, model=model)

# Or pass model name directly
metric = DeepEvalAnswerRelevancy(threshold=0.7, model="gpt-4")
```






## See Also

- [Models Documentation](./models.mdx) - Configure LLM models for
  evaluation
- [Installation](./installation.mdx) - Setup instructions
- [GitHub Repository](https://github.com/your-org/rhesis) - Source code and
  examples