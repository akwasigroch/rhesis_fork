
# Platform

The Rhesis platform provides comprehensive tools for testing and evaluating AI applications at scale.

## Why Rhesis?

Testing AI applications is fundamentally different from traditional software testing. Rhesis is purpose-built for AI testing:

- **AI-Native Testing**: Generate tests using AI, evaluate responses with LLMs
- **Scale**: Test thousands of scenarios automatically
- **Insight**: Track quality trends, catch regressions, compare models
- **Collaboration**: Multi-user workflows with roles and permissions
- **Integration**: Works with any AI model or framework

<Callout type="info">
  **New to Rhesis?** Start with [Core Concepts](/docs/getting-started/concepts) to understand how
  everything fits together, or explore the platform locally by following the [Getting
  Started](/docs/getting-started) guide.
</Callout>

## Where to Start

New to Rhesis? Follow this path:

1. **Create a [Project](/docs/platform/projects)** - Organize your testing work
2. **Configure [Endpoints](/docs/platform/endpoints)** - Connect to your AI application
3. **Generate [Tests](/docs/platform/tests-generation)** - Create test cases with AI assistance
4. **Define [Metrics](/docs/platform/metrics)** - Set up evaluation criteria
5. **Run and Analyze** - Execute tests and review results

Already familiar? Jump to any feature below.

## Core Features

<PlatformFeatures />

## Advanced Capabilities

Once you're up and running, explore these advanced features:

<AdvancedCapabilities />

---

<Callout type="default">
  **Need Help?** Check out our [Development Guide](/docs/development) for SDK and API documentation,
  or visit [Getting Started](/docs/getting-started) for initial setup.
</Callout>
