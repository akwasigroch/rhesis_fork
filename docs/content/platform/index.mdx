# Platform

The Rhesis platform provides comprehensive tools for testing and evaluating AI applications at scale.

## Why Rhesis?

Testing AI applications is fundamentally different from traditional software testing. Rhesis is purpose-built for AI testing:

- **AI-Native Testing**: Generate tests using AI, evaluate responses with LLMs
- **Scale**: Test thousands of scenarios automatically
- **Insight**: Track quality trends, catch regressions, compare models
- **Collaboration**: Multi-user workflows with roles and permissions
- **Integration**: Works with any AI model or framework

<Callout type="info">
  **New to Rhesis?** Start with [Core Concepts](/docs/getting-started/concepts) to understand how
  everything fits together, or explore the platform locally by following the [Getting
  Started](/docs/getting-started) guide.
</Callout>

## Where to Start

New to Rhesis? Follow this path:

1. **Create a [Project](/docs/platform/projects)** - Organize your testing work
2. **Configure [Endpoints](/docs/platform/endpoints)** - Connect to your AI application
3. **Generate [Tests](/docs/platform/tests-generation)** - Create test cases with AI assistance
4. **Define [Metrics](/docs/platform/metrics)** - Set up evaluation criteria
5. **Run and Analyze** - Execute tests and review results

Already familiar? Jump to any feature below.

## Core Features

<div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
  <div>
    <h3>Organizations & Team</h3>
    <div>
      Manage organization settings, invite team members, and configure contact information and
      preferences.
    </div>
  </div>
  <div>
    <h3>Projects</h3>
    <div>
      Organize testing work into projects with environment management, visual icons, and
      comprehensive project settings.
    </div>
  </div>
  <div>
    <h3>Endpoints</h3>
    <div>
      Configure API endpoints that your tests execute against, with support for REST and WebSocket
      protocols.
    </div>
  </div>
  <div>
    <h3>Tests</h3>
    <div>
      Create and manage test cases manually or generate them using AI with document context and
      iterative feedback.
    </div>
  </div>
  <div>
    <h3>Test Sets</h3>
    <div>
      Organize tests into collections and execute them against endpoints with parallel or sequential
      execution modes.
    </div>
  </div>
  <div>
    <h3>Test Runs</h3>
    <div>
      View execution results for individual test runs with filtering, comparison, and detailed
      metric analysis.
    </div>
  </div>
  <div>
    <h3>Test Results</h3>
    <div>
      Dashboard for analyzing test result trends, metrics performance, and historical data with
      advanced filtering.
    </div>
  </div>
  <div>
    <h3>Metrics</h3>
    <div>
      Define and manage LLM-based evaluation criteria with behaviors, scoring types, and
      model-driven grading.
    </div>
  </div>
  <div>
    <h3>Integrations</h3>
    <div>Connect with your existing development workflow and external services.</div>
  </div>
</div>

## Advanced Capabilities

Once you're up and running, explore these advanced features:

- **[Test Runs](/docs/platform/test-runs)** - Deep analysis with comparison and filtering
- **[Test Results](/docs/platform/test-results)** - Aggregate analytics and trend visualization
- **[Test Sets](/docs/platform/test-sets-runs)** - Organize and execute test collections
- **[API Tokens](/docs/platform/integrations/api-tokens)** - Programmatic access via [Python SDK](/docs/sdk/quickstart/sdk)
- **[Organizations](/docs/platform/organizations)** - Team management and access control

---

<Callout type="default">
  **Need Help?** Check out our [Development Guide](/docs/development) for SDK and API documentation,
  or visit [Getting Started](/docs/getting-started) for initial setup.
</Callout>
