# Projects

Organize and manage your testing projects with comprehensive project management capabilities.

<Callout type="default">
  **What are Projects?** Projects are the top-level organizational unit in Rhesis. They group
  related endpoints, tests, test sets, and results together, making it easy to manage different AI
  applications or testing initiatives.
</Callout>

## Why Projects?

Projects solve the organizational challenge of testing multiple AI applications at scale. Without projects, tests, endpoints, and results become scattered and hard to manage. Projects provide:

- **Isolation**: Keep testing work for different applications separate
- **Context**: All related tests, endpoints, and results in one place
- **Collaboration**: Clear ownership and team access per application
- **Environment Management**: Track development, staging, and production separately
- **Historical Tracking**: Understand how each AI application evolves over time

## Understanding Projects

A project in Rhesis represents a specific AI application or testing initiative you want to evaluate. Think of a project as a container that brings together everything related to testing a particular AI system. Within a project, you'll find the endpoints that connect to your AI services, the tests you've created to evaluate behavior, test sets that organize those tests for execution, and all the historical results from your test runs.

## Creating a Project

### Project Setup Wizard

Creating a new project is a guided two-step process that helps you configure everything needed to start testing.

**Step 1: Project Details**

Begin by selecting an owner for the project—this is the team member who will be primarily responsible for it. You'll also choose a visual icon to help identify your project at a glance. The platform offers over 20 icons representing different types of applications, from AI assistants and chatbots to web apps, mobile apps, databases, and cloud services. Pick one that best represents your project's purpose.

Give your project a descriptive name that makes it easy to identify, such as "Customer Support Chatbot" or "Content Generation API." You can also add a detailed description explaining what you're testing and why, which helps team members understand the project's scope and objectives.

**Step 2: Review and Create**

The final step shows you a summary of all your project details. Review everything to make sure it's correct—though don't worry, you can always edit these settings later from the project details page.

### Quick Start After Creation

Once your project is created, you'll typically:

1. Add endpoints that connect to your AI application
2. Create or generate tests
3. Organize tests into test sets
4. Run tests and analyze results

## Managing Projects

### Viewing Projects

The Projects page displays all your projects in a grid layout, giving you a quick overview of your organization's testing initiatives. Each project card shows the icon, name, description, owner, status, environment, use case, tags, and creation date.

**[SCREENSHOT HERE: Projects grid page showing multiple project cards with different icons, statuses (Active/Inactive), and environments (Development/Staging/Production). Highlight the View button on one card.]**

Click the **View** button on any project card to access the full project details page.

### Project Details Page

The project details page provides a comprehensive overview of everything related to your project. At the top, you'll see the project icon and name alongside a status badge indicating whether the project is active or inactive. If you have the appropriate permissions, you'll also see Edit and Delete buttons here.

The main content area displays the full project description, owner information with their profile picture, and tags showing the environment and use case. You'll also see when the project was created and any custom tags that have been assigned.

From this page, you can edit project settings, delete the project if necessary, or navigate to project resources like endpoints, tests, and test sets.

### Editing Projects

To modify project settings, open the project details page and click **Edit Project**. You can update the owner, icon, name, description, status, environment, use case, and tags. Changes save immediately and apply to all team members.

**[SCREENSHOT HERE: Project edit drawer open, showing form fields for editing project details including the icon selector, environment dropdown, and Active Project toggle switch.]**

### Project Status

Projects can be either active or inactive, and this status controls how team members can interact with them.

**Active Projects** are fully operational. You can create new tests, run test suites, and they're visible to all team members in dashboards and reports. Use this status for projects that are currently being developed or maintained.

**Inactive Projects** are archived but not deleted. All historical data remains accessible for review and analysis, but you can't create new tests or execute existing ones. This status is useful for completed projects or deprecated applications where you want to preserve the testing history without allowing new work.

You can toggle a project's status anytime from the project edit drawer using the "Active Project" switch.

### Deleting Projects

<Callout type="warning">
  **Important**: Deleting a project is permanent. Consider marking it inactive instead if you want
  to preserve historical data while preventing new work.
</Callout>

To delete a project, open the project details page, click **Delete**, and confirm in the modal dialog. This permanently removes the project and all associated endpoints, tests, and test sets.

## Project Organization

### Environments

Use environments to manage different deployment stages and signal the maturity level of your testing work.

**Development** is your safe space for experimentation. This is where you do local testing, rapid iteration, and configuration tuning. It's ideal for trying new testing approaches and developing experimental tests without any risk to production systems.

**Staging** represents the pre-production phase. Use this environment for integration testing with other services, performance benchmarking, and final validation checks before promoting your testing configuration to production.

**Production** is for live application testing. Here you're running regression tests against production APIs, monitoring quality in real-time, and using production-ready test configurations. Tests in production environments require careful attention and should be well-validated.

Environment tags help you quickly identify which projects require extra care and which are safe for experimentation, making it easier to navigate your testing landscape.

### Use Cases

Categorize projects by their AI application type to help team members quickly understand what each project is for.

**Chatbot** projects typically involve customer support bots, information retrieval assistants, or other conversational interfaces where the AI engages in dialogue with users.

**Assistant** projects cover productivity tools, task automation helpers, and personal AI assistants that help users accomplish specific tasks or workflows.

**Advisor** projects include decision support systems, recommendation engines, and expert advisory services where the AI provides guidance or recommendations based on complex analysis.

These categories are flexible labels that help you organize and navigate your projects more effectively.

### Tags

Create custom tags to organize projects in ways that make sense for your team. You might tag projects by team or department, client or customer, technology stack, testing priority, compliance requirements, or development phase. Add as many tags as you need to any project, then use filters and search to quickly find related projects across your organization.

---

<Callout type="default">
  **Next Steps** - Add [Endpoints](/docs/platform/endpoints) to connect your AI application - Create
  [Tests](/docs/platform/tests-generation) to validate your AI behavior - Organize tests into [Test
  Sets](/docs/platform/test-sets-runs) for execution
</Callout>
