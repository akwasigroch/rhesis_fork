# Telemetry System

Rhesis includes a privacy-focused telemetry system to collect and analyze usage patterns from both cloud-hosted and self-hosted instances. This helps us understand how the platform is used and improve it for everyone.

<Callout type="info">
**Privacy First**: All telemetry is optional, privacy-focused, and transparent. User and organization IDs are hashed using SHA-256, and sensitive data like passwords and API keys are automatically filtered out.
</Callout>

## Architecture Overview

The telemetry system consists of three main components working together:

```
┌────────────────────────────────────────────────────────┐
│  Rhesis Instances (Cloud + Self-Hosted)                │
│  ┌──────────┐              ┌──────────┐                │
│  │ Backend  │              │ Frontend │                │
│  └────┬─────┘              └────┬─────┘                │
│       │ OTLP (gRPC/HTTP)        │                      │
└───────┼─────────────────────────┼──────────────────────┘
        │                         │
        └──────────┬──────────────┘
                   ▼
        ┌──────────────────────┐
        │ OpenTelemetry        │  Port 4317 (gRPC)
        │ Collector            │  Port 4318 (HTTP)
        └──────────┬───────────┘
                   │ OTLP gRPC
                   ▼
        ┌──────────────────────┐
        │ Telemetry            │  Port 4317 (gRPC)
        │ Processor            │  Process & Store
        └──────────┬───────────┘
                   │ SQL
                   ▼
        ┌──────────────────────┐
        │ PostgreSQL           │  Separate Analytics DB
        │ Analytics Database   │  (Isolated from main DB)
        └──────────────────────┘
```

### Components

1. **OpenTelemetry Collector** - Receives telemetry from user instances, filters sensitive data, and forwards to the processor
2. **Telemetry Processor** - gRPC service that processes traces and stores structured analytics data
3. **Analytics Database** - Separate PostgreSQL database for analytics, isolated from operational data

## What Data We Collect

<Callout type="default">
The telemetry system collects **usage patterns** to help us understand how Rhesis is used and identify areas for improvement.
</Callout>

### User Activity
- Login/logout events
- Session duration
- Deployment type (cloud or self-hosted)
- Hashed user and organization IDs (SHA-256, irreversible)

### Endpoint Usage
- API endpoint paths and HTTP methods
- Response status codes
- Request duration (performance metrics)
- Timestamp of requests

### Feature Usage
- Feature interactions (created, viewed, updated, deleted)
- Feature names (e.g., "test-run", "test-set", "endpoint")
- Usage timestamps
- Deployment context

## What We DON'T Collect

<Callout type="warning">
**Privacy Protection**: The following data is automatically filtered and NEVER stored:
</Callout>

- ❌ Passwords or password hashes
- ❌ API keys or tokens
- ❌ Authentication credentials
- ❌ Personal Identifiable Information (PII)
- ❌ Test content or user-generated data
- ❌ Email addresses or usernames
- ❌ IP addresses or device identifiers
- ❌ Any sensitive business data

### ID Hashing

All user and organization IDs are one-way hashed before storage:

```python
# Example: SHA-256 hash truncated to 16 characters
hash = hashlib.sha256(id_str.encode()).hexdigest()[:16]
# Original ID: "user-123-456-789"
# Stored hash: "a1b2c3d4e5f6g7h8" (cannot be reversed)
```

**Properties:**
- ✅ **One-way**: Cannot recover original IDs
- ✅ **Consistent**: Same ID always produces the same hash for analytics
- ✅ **Anonymous**: No PII stored
- ✅ **Collision-resistant**: 2^64 unique values

## Privacy & Security

### Opt-In/Opt-Out
- Telemetry respects user preferences
- Can be disabled entirely for self-hosted instances
- No data collection without explicit configuration

### Data Isolation
- Analytics database is **completely separate** from operational data
- Different access controls and backup policies
- Can be managed independently
- No impact on application performance

### Security Measures
- Sensitive attributes filtered at the collector level
- Batch processing with retry logic for reliability
- Memory limits to prevent resource exhaustion
- Health checks and monitoring built-in

## Ports & Endpoints

### OpenTelemetry Collector

- **4317**: OTLP gRPC receiver (primary)
- **4318**: OTLP HTTP receiver (web apps)
- **8888**: Collector metrics (Prometheus)
- **13133**: Health check endpoint
- **55679**: Debug zpages

### Telemetry Processor

- **4317**: gRPC server for receiving traces from collector

## Database Schema

The analytics database uses three tables with consistent structure:

### `user_activity`
Tracks user engagement events

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `user_id` | VARCHAR(32) | Hashed user ID |
| `organization_id` | VARCHAR(32) | Hashed org ID |
| `event_type` | VARCHAR(50) | Event type (login, logout) |
| `timestamp` | TIMESTAMP | Event time |
| `session_id` | VARCHAR(255) | Session identifier |
| `deployment_type` | VARCHAR(50) | cloud / self-hosted |
| `event_metadata` | JSONB | Additional context |

### `endpoint_usage`
Tracks API usage and performance

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `endpoint` | VARCHAR(255) | API endpoint path |
| `method` | VARCHAR(10) | HTTP method |
| `user_id` | VARCHAR(32) | Hashed user ID |
| `organization_id` | VARCHAR(32) | Hashed org ID |
| `status_code` | INTEGER | HTTP status |
| `duration_ms` | DOUBLE PRECISION | Request duration |
| `timestamp` | TIMESTAMP | Request time |
| `deployment_type` | VARCHAR(50) | cloud / self-hosted |
| `event_metadata` | JSONB | Additional context |

### `feature_usage`
Tracks feature-specific interactions

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `feature_name` | VARCHAR(100) | Feature identifier |
| `user_id` | VARCHAR(32) | Hashed user ID |
| `organization_id` | VARCHAR(32) | Hashed org ID |
| `action` | VARCHAR(100) | Action type |
| `timestamp` | TIMESTAMP | Action time |
| `deployment_type` | VARCHAR(50) | cloud / self-hosted |
| `event_metadata` | JSONB | Additional context |

## Configuration

### OpenTelemetry Collector

Configured via `apps/otel-collector/otel-collector-config.yaml`:

**Key Features:**
- OTLP gRPC and HTTP receivers with CORS support
- Batch processing for efficiency
- Memory limits to prevent OOM
- Automatic sensitive data filtering
- Resource metadata enrichment
- Event categorization for analytics

**Environment Variables:**
```bash
TELEMETRY_PROCESSOR_ENDPOINT=telemetry-processor:4317
```

### Telemetry Processor

**Required Environment Variables:**
```bash
# Analytics Database (separate from main database)
ANALYTICS_DB_USER=your-db-username
ANALYTICS_DB_PASS=your-secure-password
ANALYTICS_DB_HOST=postgres-analytics
ANALYTICS_DB_PORT=5432
ANALYTICS_DB_NAME=rhesis_analytics

# Or use connection string
ANALYTICS_DATABASE_URL=postgresql://user:pass@host:port/dbname

# Service Configuration
PORT=4317
LOG_LEVEL=INFO
```

## Local Development

### Starting the Services

```bash
# Start analytics database and telemetry services
docker-compose up -d postgres-analytics otel-collector telemetry-processor

# View logs
docker-compose logs -f otel-collector telemetry-processor

# Check health
curl http://localhost:13133  # OTel Collector health
```

### Database Setup

The analytics database uses **Alembic** for migrations, which run automatically on container start.

```bash
# Manual migration
cd apps/telemetry-processor
./migrate.sh

# Or use alembic directly
alembic upgrade head

# Check current schema version
alembic current
```

### Connect to Analytics Database

```bash
# Via Docker Compose
docker-compose exec postgres-analytics psql -U your-db-username -d rhesis_analytics

# Check data
SELECT COUNT(*) FROM user_activity;
SELECT COUNT(*) FROM endpoint_usage;
SELECT COUNT(*) FROM feature_usage;
```

## Monitoring & Analytics

### Health Checks

```bash
# OTel Collector health
curl http://localhost:13133

# Collector metrics
curl http://localhost:8888/metrics

# Debug traces
open http://localhost:55679/debug/tracez
```

### Sample Queries

**Recent Activity Summary:**
```sql
SELECT 
    event_type,
    COUNT(*) as count,
    MAX(timestamp) as latest
FROM user_activity
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY event_type;
```

**Top Endpoints by Usage:**
```sql
SELECT 
    endpoint,
    method,
    COUNT(*) as hits,
    AVG(duration_ms) as avg_duration_ms
FROM endpoint_usage
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY endpoint, method
ORDER BY hits DESC
LIMIT 10;
```

**Feature Adoption:**
```sql
SELECT 
    feature_name,
    action,
    COUNT(*) as count
FROM feature_usage
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature_name, action
ORDER BY count DESC;
```

**Overall Statistics:**
```sql
SELECT 
    'user_activity' as table_name, 
    COUNT(*) as count 
FROM user_activity
UNION ALL
SELECT 'endpoint_usage', COUNT(*) FROM endpoint_usage
UNION ALL
SELECT 'feature_usage', COUNT(*) FROM feature_usage;
```

## Troubleshooting

### No Data Appearing

1. **Check telemetry is enabled:**
   ```bash
   # In your .env file
   TELEMETRY_ENABLED=true
   DEPLOYMENT_TYPE=self-hosted  # or "cloud"
   ```

2. **Verify OTel Collector is receiving data:**
   ```bash
   docker-compose logs otel-collector
   # Look for "Traces received" messages
   ```

3. **Check Telemetry Processor logs:**
   ```bash
   docker-compose logs telemetry-processor
   # Look for "Processing span" messages
   ```

4. **Test database connection:**
   ```bash
   docker-compose exec postgres-analytics psql -U user -d dbname -c "SELECT 1;"
   ```

### Connection Issues

**OTel Collector can't reach Telemetry Processor:**
```bash
# Check if processor is running
docker-compose ps telemetry-processor

# Check network connectivity
docker-compose exec otel-collector ping telemetry-processor
```

**Processor can't reach database:**
```bash
# Verify database is running
docker-compose ps postgres-analytics

# Check logs
docker-compose logs postgres-analytics
```

### Performance Issues

1. **Check database indexes:**
   ```sql
   SELECT tablename, indexname 
   FROM pg_indexes 
   WHERE tablename IN ('user_activity', 'endpoint_usage', 'feature_usage');
   ```

2. **Monitor active connections:**
   ```sql
   SELECT COUNT(*), state 
   FROM pg_stat_activity 
   WHERE datname = 'rhesis_analytics'
   GROUP BY state;
   ```

3. **Review collector memory usage:**
   ```bash
   docker stats otel-collector telemetry-processor
   ```

## Deployment

### Google Cloud Run

Both services can be deployed to Cloud Run with Cloud SQL for the analytics database.

**Deployment workflows:**
- `.github/workflows/otel-collector.yml` - OTel Collector deployment
- `.github/workflows/telemetry-processor.yml` - Processor deployment

**Cloud SQL Connection:**
```bash
ANALYTICS_DATABASE_URL=postgresql://user:pass@/dbname?host=/cloudsql/PROJECT:REGION:INSTANCE
```

### Kubernetes

Kubernetes manifests are available:
```
infrastructure/k8s/manifests/otel-collector-deployment.yaml
```

## Testing Locally

### Build and Run Collector

```bash
cd apps/otel-collector
docker build -t rhesis-otel-collector .

docker run -p 4317:4317 -p 4318:4318 -p 13133:13133 \
  -e TELEMETRY_PROCESSOR_ENDPOINT=host.docker.internal:4317 \
  rhesis-otel-collector
```

### Build and Run Processor

```bash
cd apps/telemetry-processor
docker build -t rhesis-telemetry-processor .

docker run -p 4317:4317 \
  -e ANALYTICS_DATABASE_URL=postgresql://user:pass@host/db \
  rhesis-telemetry-processor
```

## Development

### Project Structure

**OpenTelemetry Collector:**
```
apps/otel-collector/
├── otel-collector-config.yaml    # Collector configuration
├── Dockerfile                     # Container image
└── README.md                      # Component docs
```

**Telemetry Processor:**
```
apps/telemetry-processor/
├── src/processor/
│   ├── main.py                   # Entry point
│   ├── models/analytics.py       # Database models
│   ├── database/connection.py    # DB connection
│   ├── services/                 # Span processors
│   │   ├── user_activity.py
│   │   ├── endpoint_usage.py
│   │   ├── feature_usage.py
│   │   └── span_router.py
│   └── grpc/trace_service.py     # gRPC implementation
├── alembic/                       # Database migrations
├── Dockerfile                     # Container image
└── pyproject.toml                 # Dependencies
```

### Adding New Analytics

1. **Add model** to `models/analytics.py`:
   ```python
   class NewFeatureAnalytics(AnalyticsBase):
       __tablename__ = "new_feature_analytics"
       custom_field = Column(String(100))
   ```

2. **Create migration**:
   ```bash
   cd apps/telemetry-processor
   alembic revision -m "add new feature analytics"
   ```

3. **Add processor service** in `services/`:
   ```python
   class NewFeatureProcessor(BaseSpanProcessor):
       def process_span(self, span, resource, session):
           # Processing logic
   ```

4. **Update router** in `services/span_router.py`

5. **Run migration**:
   ```bash
   alembic upgrade head
   ```

## Why Separate Analytics Database?

<Callout type="default">
The telemetry processor uses a **dedicated PostgreSQL database** separate from the main application database.
</Callout>

**Benefits:**

✅ **Isolation** - Analytics doesn't affect operational database performance  
✅ **Security** - Different access controls and permissions  
✅ **Scalability** - Scale independently based on analytics volume  
✅ **Backup** - Different retention policies for analytics vs operational data  
✅ **Privacy** - Easier to manage compliance and data retention  
✅ **Performance** - Optimized for analytics workloads

## Learn More

For implementation details, see:
- `apps/otel-collector/otel-collector-config.yaml` - Collector configuration
- `apps/telemetry-processor/src/` - Processor source code
- `apps/telemetry-processor/alembic/versions/` - Database migrations

